# EYE FOR Blind
## Problem statement

A significant portion of the global population, approximately 285 million people, faces visual impairment, with 39 million of them being completely blind. These individuals encounter substantial challenges in carrying out daily activities that rely on visual information, such as reading text (e.g., newspapers, messages) or understanding visual content like pictures and images. In today's digital and social media-driven world, where millions of images are shared daily, visually impaired and blind people face barriers in accessing and appreciating visual content, limiting their ability to engage with information, enjoy visual media, and fully participate in social and cultural activities.

This calls for the development of solutions that can help these individuals understand and interact with both text and image-based content more effectively.

## Solution

Make a model that is similar to the one developed by Facebook, specifically such that a blind person knows the contents of an image in front of them with the help of a CNN-RNN based model. The model will convert the contents of an image and will provide the output in the form of audio.

Image-to-caption generation which is based on the popular <a href="https://arxiv.org/pdf/1502.03044">Show, Attend and Tell paper.</a>

<h4>Encoder</h4>
The encoder parts involve the convolution of the input image with the help of various convolution, max pooling, and fully connected layers. Since we are not dealing with the classification of the image, we have removed them from the end. The final output of the encoder part will be the generation of the feature vector.

<h4>Attention model</h4>
Instead of passing the complete input image to the RNN at every timestamp, different relevant parts of the image is passed to it. With an attention model, we get a varying feature vector based on the previously generated word. This helps to look at different parts of the image for better captioning.

<h4>Decoder</h4>
The decoder predicts the word based on context vector generated by Attention model, along with hidden state from previous timestamp. The word is generated till it reaches the <b>end</b> tag.
